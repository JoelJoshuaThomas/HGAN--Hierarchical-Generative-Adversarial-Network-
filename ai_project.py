{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13900910,"sourceType":"datasetVersion","datasetId":8856410}],"dockerImageVersionId":31192,"isInternetEnabled":false,"language":"python","sourceType":"script","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# %% [code]\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-11-27T21:15:47.72874Z\",\"iopub.execute_input\":\"2025-11-27T21:15:47.729028Z\",\"iopub.status.idle\":\"2025-11-27T21:16:39.32637Z\",\"shell.execute_reply.started\":\"2025-11-27T21:15:47.72898Z\",\"shell.execute_reply\":\"2025-11-27T21:16:39.325158Z\"}}\n!pip uninstall -y numpy scipy scikit-learn threadpoolctl joblib\n!pip install --no-cache-dir numpy==1.26.4 scipy==1.11.4 scikit-learn==1.4.2 threadpoolctl joblib\n\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-11-27T21:17:50.11026Z\",\"iopub.execute_input\":\"2025-11-27T21:17:50.110615Z\",\"iopub.status.idle\":\"2025-11-27T21:18:03.025504Z\",\"shell.execute_reply.started\":\"2025-11-27T21:17:50.110577Z\",\"shell.execute_reply\":\"2025-11-27T21:18:03.024313Z\"}}\n!pip install torch-scatter torch-sparse torch-cluster torch-spline-conv torch-geometric -f https://data.pyg.org/whl/torch-2.1.0+cu121.html\n\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-11-27T21:18:03.02726Z\",\"iopub.execute_input\":\"2025-11-27T21:18:03.027621Z\",\"iopub.status.idle\":\"2025-11-27T21:18:14.806686Z\",\"shell.execute_reply.started\":\"2025-11-27T21:18:03.027581Z\",\"shell.execute_reply\":\"2025-11-27T21:18:14.805595Z\"}}\nimport torch_geometric\nprint(torch_geometric.__version__)\n\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-11-27T21:20:01.370942Z\",\"iopub.execute_input\":\"2025-11-27T21:20:01.371554Z\",\"iopub.status.idle\":\"2025-11-27T21:20:20.79487Z\",\"shell.execute_reply.started\":\"2025-11-27T21:20:01.371524Z\",\"shell.execute_reply\":\"2025-11-27T21:20:20.794048Z\"}}\n# ============================================================\n# KAGGLE-SAFE HIERARCHICAL GAN STARTER BLOCK (Multi-Epoch, RMSE-Improved)\n# ============================================================\n\nimport pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch_geometric.nn import SAGEConv\nimport random\n\n# -------------------------------\n# 1. SEED & DEVICE\n# -------------------------------\ndef seed_everything(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\nseed_everything()\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# -------------------------------\n# 2. LOAD DATA & SAMPLE\n# -------------------------------\nsales = pd.read_csv(\"/kaggle/input/walmart8/sales_train_validation.csv\")\ncalendar = pd.read_csv(\"/kaggle/input/walmart8/calendar.csv\")\nprices = pd.read_csv(\"/kaggle/input/walmart8/sell_prices.csv\")\n\n# Sample SKUs to reduce memory\nsample_skus = sales['id'].sample(n=300, random_state=42).values\nsales_sample = sales[sales['id'].isin(sample_skus)].copy()\n\n# Take only last N days safely\nlast_n_days = 200\nall_day_cols = sales_sample.columns[1:]  # exclude 'id'\nsales_sample = sales_sample[['id'] + list(all_day_cols[-last_n_days:])]\n\n# -------------------------------\n# 3. HIERARCHICAL MAPPINGS\n# -------------------------------\nn_skus = len(sample_skus)\nn_depts = len(sales_sample['id'].apply(lambda x: x.split(\"_\")[1]).unique())\nn_categories = len(sales_sample['id'].apply(lambda x: x.split(\"_\")[0]).unique())\n\nsku2dept = sales_sample['id'].apply(lambda x: x.split(\"_\")[1]).astype(\"category\").cat.codes.values\ndept2cat = np.arange(n_depts) % n_categories  # simple dept->cat mapping for demo\n\n# -------------------------------\n# 4. SKU FEATURES (Robust price + promo)\n# -------------------------------\n\n# prices: aggregate duplicates safely\nprices_clean = prices.groupby(['item_id', 'wm_yr_wk'])['sell_price'].mean().reset_index()\n\n# Build a mapping item_id -> sorted (wk, price) numpy array for quick access\nprices_by_item = {}\nfor item_id, group in prices_clean.groupby('item_id'):\n    g = group.sort_values('wm_yr_wk')\n    prices_by_item[item_id] = (g['wm_yr_wk'].values, g['sell_price'].values.astype(float))\n\n# Create price_tensor robustly (n_skus x last_n_days)\nprice_tensor = torch.zeros(n_skus, last_n_days, dtype=torch.float32)\n\nfor i, sku in enumerate(sample_skus):\n    if sku in prices_by_item:\n        wk_arr, p_arr = prices_by_item[sku]\n        # take last_n_days observations (by week order)\n        vals = p_arr[-last_n_days:]\n        if len(vals) == 0:\n            # fallback synthetic\n            vals = (np.random.rand(last_n_days) * 5.0 + 5.0).astype(float)\n        elif len(vals) < last_n_days:\n            # pad on the left with the first observed or the median\n            pad_len = last_n_days - len(vals)\n            pad_val = np.nanmedian(vals) if len(vals)>0 else 5.0\n            vals = np.concatenate([np.full(pad_len, pad_val, dtype=float), vals])\n        price_tensor[i, :] = torch.tensor(vals, dtype=torch.float32)\n    else:\n        # no price info -> synthetic fallback\n        price_tensor[i, :] = torch.tensor((np.random.rand(last_n_days) * 5.0 + 5.0), dtype=torch.float32)\n\nprice_tensor = price_tensor.to(device)\n\n# Build promo_tensor: prefer using calendar event columns if they exist\nif 'event_name_1' in calendar.columns or 'event_name_2' in calendar.columns:\n    # Map wm_yr_wk -> any event flag\n    event_weeks = set(calendar.loc[calendar['event_name_1'].notnull(), 'wm_yr_wk'].unique()) \\\n                  | set(calendar.loc[calendar['event_name_2'].notnull(), 'wm_yr_wk'].unique())\n    # get the last N weeks from calendar (unique weeks ordered)\n    wk_order = calendar[['wm_yr_wk']].drop_duplicates().sort_index().wm_yr_wk.values\n    last_wks = wk_order[-last_n_days:]\n    promo_tensor = torch.zeros(n_skus, last_n_days, dtype=torch.float32)\n    for j, wk in enumerate(last_wks):\n        if wk in event_weeks:\n            promo_tensor[:, j] = 1.0\n    promo_tensor = promo_tensor.to(device)\nelse:\n    # synthetic sparse promos: about 2-4 promo days per horizon per SKU\n    promo_tensor = torch.zeros(n_skus, last_n_days, dtype=torch.float32)\n    for i in range(n_skus):\n        k = max(1, last_n_days // 60)\n        days = np.random.choice(np.arange(last_n_days), size=k, replace=False)\n        promo_tensor[i, days] = 1.0\n    promo_tensor = promo_tensor.to(device)\n\n# -------------------------------\n# Sales tensor (raw) and normalization\n# -------------------------------\nsales_tensor = torch.tensor(sales_sample.iloc[:, 1:].values, dtype=torch.float32).to(device)  # SKU x T\n\n# Normalize sales per SKU (if you prefer training on normalized targets)\nsales_mean = sales_tensor.mean(dim=1, keepdim=True)\nsales_std = sales_tensor.std(dim=1, keepdim=True) + 1e-6\nsales_tensor_norm = (sales_tensor - sales_mean) / sales_std\n\n# Normalize prices per SKU\nprice_mean = price_tensor.mean(dim=1, keepdim=True)\nprice_std = price_tensor.std(dim=1, keepdim=True) + 1e-6\nprice_tensor_norm = (price_tensor - price_mean) / price_std\n\n# -------------------------------\n# 5. GRAPH EDGE INDEX (for GNN)\n# -------------------------------\nedges = []\nfor i in range(n_skus):\n    for j in range(i+1, n_skus):\n        if sku2dept[i] == sku2dept[j]:  # same dept = correlated\n            edges.append([i, j])\nif len(edges) == 0:\n    # safe fallback (fully connect small set)\n    edges = [[i, j] for i in range(n_skus) for j in range(i+1, n_skus)]\nedge_index = torch.tensor(edges, dtype=torch.long).T.to(device)\n\n# -------------------------------\n# 6. HIERARCHICAL S MATRICES (Normalized for Category Forecast)\n# -------------------------------\nS_sku2dept = torch.zeros(n_depts, n_skus)\nfor i in range(n_skus): S_sku2dept[sku2dept[i], i] = 1\nS_dept2cat = torch.zeros(n_categories, n_depts)\nfor i in range(n_depts): S_dept2cat[dept2cat[i], i] = 1\nS_sku2cat = S_dept2cat @ S_sku2dept\nS_sku2cat_norm = S_sku2cat / (S_sku2cat.sum(1, keepdim=True) + 1e-6)  # safe normalization\nS_sku2cat_norm = S_sku2cat_norm.to(device)\n\n# -------------------------------\n# 7. MODEL CLASSES\n# -------------------------------\nclass GlobalGenerator(nn.Module):\n    def __init__(self, n_categories, hidden=32):\n        super().__init__()\n        # input_size for LSTM = n_categories (we feed category-level multivariate sequence)\n        self.lstm = nn.LSTM(n_categories, hidden, batch_first=True)\n        self.attn = nn.MultiheadAttention(hidden, num_heads=4, batch_first=True)\n        self.fc = nn.Linear(hidden, n_categories)\n    def forward(self, x):\n        # x: (batch=1, seq_len, n_categories)\n        lstm_out,_ = self.lstm(x)\n        attn_out,_ = self.attn(lstm_out, lstm_out, lstm_out)\n        return self.fc(attn_out[:, -1, :])   # returns (batch=1, n_categories)\n\nclass LocalGenerator(nn.Module):\n    def __init__(self, n_skus, global_size, hidden=32):\n        super().__init__()\n        self.fc1 = nn.Linear(global_size + 2, hidden)\n        self.gnn = SAGEConv(hidden, hidden)\n        self.fc2 = nn.Linear(hidden, 1)\n        self.n_skus = n_skus\n    def forward(self, global_forecast, price, promo, edge_index):\n        # global_forecast expected shape: (1, n_categories) or (n_categories,)\n        if global_forecast.dim() == 2 and global_forecast.size(0) == 1:\n            g = global_forecast.squeeze(0)  # (n_categories,)\n        else:\n            g = global_forecast\n        # price, promo: (n_skus,)\n        x = torch.stack([price, promo], dim=1)          # (n_skus, 2)\n        global_repeat = g.unsqueeze(0).repeat(self.n_skus, 1)  # (n_skus, n_categories)\n        x = torch.cat([x, global_repeat], dim=1)        # (n_skus, n_categories+2)\n        x = torch.relu(self.fc1(x))\n        x = self.gnn(x, edge_index)                     # (n_skus, hidden)\n        return self.fc2(x)                              # (n_skus, 1)\n\nclass Discriminator(nn.Module):\n    def __init__(self, input_size, hidden=32):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(input_size, hidden),\n            nn.ReLU(),\n            nn.Linear(hidden, 1)\n        )\n    def forward(self, x):\n        return self.net(x)\n\nclass ReconciliationLayer(nn.Module):\n    def __init__(self, S):\n        super().__init__()\n        self.S = S\n    def forward(self, y_sku):\n        # y_sku: (n_skus,) or (n_skus,1)\n        y = y_sku.squeeze(-1) if y_sku.dim() == 2 and y_sku.size(1) == 1 else y_sku\n        return self.S @ y  # (n_categories,)\n\n# -------------------------------\n# 8. WGAN-GP UTILS\n# -------------------------------\ndef gradient_penalty(D, real, fake):\n    # real/fake shapes: (batch, features)\n    alpha = torch.rand(real.size(0), 1, device=device)\n    alpha = alpha.expand_as(real)\n    interpolated = (alpha * real + (1 - alpha) * fake).requires_grad_(True)\n    pred = D(interpolated)\n    grads = torch.autograd.grad(outputs=pred, inputs=interpolated,\n                                grad_outputs=torch.ones_like(pred),\n                                create_graph=True, retain_graph=True)[0]\n    return ((grads.norm(2, dim=1) - 1) ** 2).mean()\n\n# -------------------------------\n# 9. INITIALIZE MODELS + OPTIMIZERS\n# -------------------------------\nGg = GlobalGenerator(n_categories).to(device)\nGl = LocalGenerator(n_skus, n_categories).to(device)\nDg = Discriminator(n_categories).to(device)\nDl = Discriminator(1).to(device)\nrecon_layer = ReconciliationLayer(S_sku2cat_norm.to(device))\n\nopt_Gg = optim.Adam(Gg.parameters(), lr=1e-3)\nopt_Gl = optim.Adam(Gl.parameters(), lr=1e-3)\nopt_Dg = optim.Adam(Dg.parameters(), lr=1e-3)\nopt_Dl = optim.Adam(Dl.parameters(), lr=1e-3)\n\n# -------------------------------\n# 10. PREP INPUTS FOR TRAINING\n# -------------------------------\n# Build category-level sequences from normalized sales (we train global on normalized)\nsales_cat = torch.zeros(n_categories, last_n_days, dtype=torch.float32).to(device)\nfor c in range(n_categories):\n    idxs = np.where(dept2cat[sku2dept] == c)[0]\n    if len(idxs) > 0:\n        sales_cat[c] = sales_tensor_norm[idxs].sum(0)\n\nx_seq_input = sales_cat.T.unsqueeze(0)  # (1, seq_len, n_categories)\ny_real_global = sales_cat[:, -1].unsqueeze(0)  # (1, n_categories)\n\n# Last-step price/promo (normalized price)\nprice_last = price_tensor_norm[:, -1].to(device)   # (n_skus,)\npromo_last = promo_tensor[:, -1].to(device)        # (n_skus,)\n\n# -------------------------------\n# 11. TRAINING LOOP (unchanged logic, but tunable coherence)\n# -------------------------------\nn_epochs = 200\nn_critic = 5  # Discriminator updates per Generator update\n\n# Expose coherence weight for tuning (fix B)\ncoherence_weight = 1.0   # try 0.1, 1.0, 5.0 in experiments\n\nfor epoch in range(n_epochs):\n\n    # ----- Train Global Discriminator -----\n    for _ in range(n_critic):\n        y_fake_global = Gg(x_seq_input)  # (1, n_categories)\n        D_real = Dg(y_real_global)       # (1,1) outputs\n        D_fake = Dg(y_fake_global.detach())\n        gp = gradient_penalty(Dg, y_real_global, y_fake_global.detach())\n        loss_Dg = -torch.mean(D_real) + torch.mean(D_fake) + 10.0 * gp\n\n        opt_Dg.zero_grad()\n        loss_Dg.backward()\n        opt_Dg.step()\n\n    # ----- Train Global Generator -----\n    y_fake_global = Gg(x_seq_input)\n    loss_Gg = -torch.mean(Dg(y_fake_global))\n    opt_Gg.zero_grad()\n    loss_Gg.backward(retain_graph=True)\n    opt_Gg.step()\n\n    # ----- Train Local Discriminator -----\n    # create a detached global vector for local generator (use as context)\n    y_fake_global_local = y_fake_global.clone().detach()  # (1, n_categories)\n    # compute local fake (use normalized price_last / promo_last)\n    y_fake_local = Gl(y_fake_global_local, price_last, promo_last, edge_index)  # (n_skus,1)\n\n    for _ in range(n_critic):\n        D_real_local = Dl(sales_tensor[:, -1].unsqueeze(-1))   # true SKU last-step (raw scale)\n        D_fake_local = Dl(y_fake_local.detach())\n        gp_local = gradient_penalty(Dl, sales_tensor[:, -1].unsqueeze(-1), y_fake_local.detach())\n        loss_Dl = -torch.mean(D_real_local) + torch.mean(D_fake_local) + 10.0 * gp_local\n\n        opt_Dl.zero_grad()\n        loss_Dl.backward()\n        opt_Dl.step()\n\n    # ----- Train Local Generator -----\n    # reconcile SKU -> category\n    y_recon = recon_layer(y_fake_local.squeeze(-1))  # (n_categories,)\n    # y_fake_global_local shape convert to (n_categories,)\n    y_fake_global_for_coh = y_fake_global_local.squeeze(0)  # (n_categories,)\n\n    loss_coh = nn.MSELoss()(y_recon, y_fake_global_for_coh)\n    # local MSE against the true last-step SKU values (raw scale) + coherence weighted term\n    loss_Gl = nn.MSELoss()(y_fake_local.squeeze(-1), sales_tensor[:, -1]) + coherence_weight * loss_coh\n\n    opt_Gl.zero_grad()\n    loss_Gl.backward()\n    opt_Gl.step()\n\n    # ----- Print every 10 epochs -----\n    if (epoch + 1) % 10 == 0:\n        print(f\"Epoch [{epoch+1}/{n_epochs}] | Dg: {loss_Dg.item():.4f} | Dl: {loss_Dl.item():.4f} | \"\n              f\"Gg: {loss_Gg.item():.4f} | Gl: {loss_Gl.item():.4f} | Coherence: {loss_coh.item():.4f}\")\n\n# -------------------------------\n# 12. FINAL FORECASTS\n# -------------------------------\nprint(\"Final Global Forecast Shape:\", y_fake_global.shape)  # (1, n_categories)\nprint(\"Final Local Forecast Shape :\", y_fake_local.shape)   # (n_skus, 1)\n\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-11-27T21:20:35.939525Z\",\"iopub.execute_input\":\"2025-11-27T21:20:35.939851Z\",\"iopub.status.idle\":\"2025-11-27T21:20:35.952759Z\",\"shell.execute_reply.started\":\"2025-11-27T21:20:35.939827Z\",\"shell.execute_reply\":\"2025-11-27T21:20:35.951843Z\"}}\n# -------------------------------\n# 13. EVALUATION METRICS (CORRECTED)\n# -------------------------------\n\n# y_fake_local : SKU-level forecast (n_skus x 1)\n# y_recon      : category-level aggregated forecast (n_categories x 1)\n# sales_tensor[:, -1] : true SKU-level values (last time step)\n# sales_cat[:, -1]    : true category-level values (last time step)\n\neps = 1e-8  # to prevent divide-by-zero everywhere\n\n# ================================================\n#  SKU-LEVEL METRICS\n# ================================================\ny_sku_true = sales_tensor[:, -1]          # shape (n_skus,)\ny_sku_pred = y_fake_local.squeeze(-1)     # shape (n_skus,)\n\nrmse_sku = torch.sqrt(((y_sku_true - y_sku_pred) ** 2).mean()).item()\nmae_sku  = torch.mean(torch.abs(y_sku_true - y_sku_pred)).item()\n\n# ---- Correct WRMSSE: denominator = sum(diff^2) ----\n# For SKU-level we need per-SKU series differences, but you only have last-step comparison.\n# So fall back to mean() denominator safely.\nsku_den = torch.mean((y_sku_true - y_sku_true.mean()) ** 2) + eps\n\nwrmsse_sku = torch.sqrt(\n    torch.mean((y_sku_true - y_sku_pred) ** 2 / sku_den)\n).item()\n\n\n# ================================================\n#  CATEGORY-LEVEL METRICS\n# ================================================\ny_cat_true = sales_cat[:, -1]           # (n_categories,)\ny_cat_pred = y_recon.squeeze(-1)        # (n_categories,)\n\nrmse_cat = torch.sqrt(((y_cat_true - y_cat_pred) ** 2).mean()).item()\nmae_cat  = torch.mean(torch.abs(y_cat_true - y_cat_pred)).item()\n\n# ---- Proper WRMSSE denominator using squared differences ----\n# Compute (y_t - y_{t-1})^2 across time for each category\n\ncat_series = sales_cat  # shape: (n_categories, T)\ncat_diffs = cat_series[:, 1:] - cat_series[:, :-1]  # (n_categories, T-1)\n\nden_cat = torch.sum(cat_diffs ** 2, dim=1)  # per-category denominator (n_categories,)\nden_cat = den_cat + eps                     # replace zeros --> no NaN\n\nnum_cat = (y_cat_true - y_cat_pred) ** 2     # numerator per category\n\nwrmsse_cat = torch.sqrt(torch.mean(num_cat / den_cat)).item()\n\n\n# ================================================\n#  PRINT\n# ================================================\nprint(\"--- SKU-level Metrics ---\")\nprint(f\"RMSE : {rmse_sku:.4f}\")\nprint(f\"MAE  : {mae_sku:.4f}\")\nprint(f\"WRMSSE : {wrmsse_sku:.4f}\")\n\nprint(\"--- Category-level Metrics ---\")\nprint(f\"RMSE : {rmse_cat:.4f}\")\nprint(f\"MAE  : {mae_cat:.4f}\")\nprint(f\"WRMSSE : {wrmsse_cat:.4f}\")\n\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-11-27T21:23:26.570828Z\",\"iopub.execute_input\":\"2025-11-27T21:23:26.571539Z\",\"iopub.status.idle\":\"2025-11-27T21:23:26.614941Z\",\"shell.execute_reply.started\":\"2025-11-27T21:23:26.571508Z\",\"shell.execute_reply\":\"2025-11-27T21:23:26.614124Z\"}}\n# ============================================================\n# 14. MULTI-STEP FORECASTING (7-DAY)\n# ============================================================\n\nforecast_horizon = 7\n\n# Start from normalized final sequences\nx_sku = sales_tensor_norm.clone()        # (n_skus, T)\nx_cat = sales_cat.clone()                # (n_categories, T)\n\nprice_hist = price_tensor_norm.clone()\npromo_hist = promo_tensor.clone()\n\nfuture_global = []\nfuture_local = []\n\nfor step in range(forecast_horizon):\n\n    # -----------------------------------------------\n    # 1. Global category forecast\n    # -----------------------------------------------\n    # Input shape for Gg: (1, seq_len, n_categories)\n    global_in = x_cat.T.unsqueeze(0)\n    global_pred = Gg(global_in)          # (1, n_categories)\n    global_pred = global_pred.squeeze(0) # (n_categories,)\n\n    future_global.append(global_pred)\n\n    # -----------------------------------------------\n    # 2. Local SKU forecast\n    # -----------------------------------------------\n    price_last_step = price_hist[:, -1]     # (n_skus,)\n    promo_last_step = promo_hist[:, -1]     # (n_skus,)\n\n    local_pred = Gl(global_pred, \n                    price_last_step, \n                    promo_last_step,\n                    edge_index)             # (n_skus, 1)\n\n    local_pred = local_pred.squeeze(-1)     # (n_skus,)\n    future_local.append(local_pred)\n\n    # -----------------------------------------------\n    # 3. Autoregressive update\n    # -----------------------------------------------\n    # SKU series\n    x_sku = torch.cat([x_sku[:, 1:], local_pred.unsqueeze(1)], dim=1)\n\n    # Category series (aggregate via S matrix)\n    next_cat = (S_sku2cat_norm.to(device) @ local_pred)\n    x_cat = torch.cat([x_cat[:, 1:], next_cat.unsqueeze(1)], dim=1)\n\n    # price & promo (repeat last value)\n    price_hist = torch.cat([price_hist[:, 1:], price_last_step.unsqueeze(1)], dim=1)\n    promo_hist = torch.cat([promo_hist[:, 1:], promo_last_step.unsqueeze(1)], dim=1)\n\n# Convert output to tensors\nfuture_global = torch.stack(future_global, dim=1)  # (n_categories, H)\nfuture_local  = torch.stack(future_local, dim=1)   # (n_skus, H)\n\nprint(\"Future Global Forecasts Shape :\", future_global.shape)\nprint(\"Future Local Forecasts Shape  :\", future_local.shape)\n\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-11-27T21:23:41.254525Z\",\"iopub.execute_input\":\"2025-11-27T21:23:41.254861Z\",\"iopub.status.idle\":\"2025-11-27T21:23:41.294676Z\",\"shell.execute_reply.started\":\"2025-11-27T21:23:41.254835Z\",\"shell.execute_reply\":\"2025-11-27T21:23:41.293653Z\"}}\n# =============================\n# Full Corrected Scenario Generation Pipeline\n# =============================\nimport torch\nimport torch.nn as nn\n\n# --------------------------------------------------\n# 1. Dummy Model (Replace with your trained model)\n# --------------------------------------------------\nclass SimpleForecastModel(nn.Module):\n    def __init__(self, input_size=3, hidden=32):\n        super().__init__()\n        self.lstm = nn.LSTM(input_size, hidden, batch_first=True)\n        self.fc = nn.Linear(hidden, 1)\n\n    def forward(self, x):\n        out, _ = self.lstm(x)\n        out = self.fc(out[:, -1, :])\n        return out.squeeze(-1)\n\nmodel = SimpleForecastModel()\n\n# --------------------------------------------------\n# 2. Helper: Generate autoregressive forecasts\n# --------------------------------------------------\ndef autoregressive_forecast(model, sales, price, promo, horizon=7):\n    model.eval()\n    preds = []\n\n    for _ in range(horizon):\n        x = torch.stack([sales[:, -1], price[:, -1], promo[:, -1]], dim=-1).unsqueeze(1)\n        y = model(x)\n        preds.append(y.detach())\n\n        # append prediction to series\n        sales = torch.cat([sales, y.unsqueeze(1)], dim=1)\n        price = torch.cat([price, price[:, -1:].clone()], dim=1)\n        promo = torch.cat([promo, promo[:, -1:].clone()], dim=1)\n\n    return torch.stack(preds, dim=1)\n\n# --------------------------------------------------\n# 3. Scenario Generator\n# --------------------------------------------------\ndef generate_scenario(model,\n                      sales_tensor,\n                      price_tensor,\n                      promo_tensor,\n                      S_sku2cat=None,\n                      edge_index=None,\n                      horizon=7,\n                      scenario=\"promo_spike\",\n                      target_skus=None,\n                      save_csv=False):\n\n    sales = sales_tensor.clone()\n    price = price_tensor.clone()\n    promo = promo_tensor.clone()\n\n    if target_skus is None:\n        target_skus = list(range(sales.size(0)))\n\n    # -------- Baseline forecast --------\n    baseline = autoregressive_forecast(model, sales, price, promo, horizon=horizon)\n\n    # -------- Scenario Modification --------\n    if scenario == \"promo_spike\":\n        promo[:, -1:] = 1  # apply spike on last known period\n\n    elif scenario == \"price_drop\":\n        price[:, -1:] = price[:, -1:] * 0.7  # 30% drop\n\n    elif scenario == \"bf_spike\":\n        sales[:, -1:] = sales[:, -1:] * 1.8  # 80% spike\n\n    # -------- Scenario Forecast --------\n    scenario_out = autoregressive_forecast(model, sales, price, promo, horizon=horizon)\n\n    # -------- Compute Percentage Change --------\n    pct_change = (scenario_out - baseline) / (baseline + 1e-6) * 100\n\n    # -------- SKU Rankings --------\n    avg_change = pct_change.mean(dim=1)\n    top = torch.argsort(avg_change, descending=True)\n\n    return {\n        \"baseline\": baseline,\n        \"scenario\": scenario_out,\n        \"pct_change\": pct_change,\n        \"top_skus\": top[:10],\n        \"avg_change\": avg_change[top[:10]]\n    }\n\n# --------------------------------------------------\n# 4. Safety Block: Ensure tensors exist\n# --------------------------------------------------\nnum_skus = 50\nT = 40\nsales_tensor = torch.rand(num_skus, T) * 100\nprice_tensor = torch.rand(num_skus, T) * 10\npromo_tensor = torch.zeros(num_skus, T)\n\nsku_seq = sales_tensor\n\n# --------------------------------------------------\n# 5. Execute Scenario\n# --------------------------------------------------\nout = generate_scenario(\n    model,\n    sales_tensor=sales_tensor,\n    price_tensor=price_tensor,\n    promo_tensor=promo_tensor,\n    S_sku2cat=None,\n    edge_index=None,\n    horizon=7,\n    scenario=\"promo_spike\",\n    target_skus=list(range(10)),\n    save_csv=False\n)\n\nprint(\"Top changed SKUs:\")\nfor idx, val in zip(out[\"top_skus\"], out[\"avg_change\"]):\n    print(int(idx), f\"{float(val):.2f}%\")\n\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-11-27T21:23:51.746064Z\",\"iopub.execute_input\":\"2025-11-27T21:23:51.746409Z\",\"iopub.status.idle\":\"2025-11-27T21:23:51.901265Z\",\"shell.execute_reply.started\":\"2025-11-27T21:23:51.74638Z\",\"shell.execute_reply\":\"2025-11-27T21:23:51.90006Z\"}}\nimport numpy as np\ndef top_changes(baseline, scenario, topk=10):\n    # baseline, scenario: numpy arrays (n_skus, H)\n    pct = (scenario - baseline) / (np.abs(baseline) + 1e-6)\n    avg_pct = pct.mean(axis=1)   # average impact across horizon\n    idx = np.argsort(-np.abs(avg_pct))[:topk]\n    return idx, avg_pct[idx]\n\n# example:\nbase = baseline[\"sku_forecasts\"]        # (n_skus, H)\npromo = promo_spike[\"sku_forecasts\"]\nidx, avg_pct = top_changes(base, promo, topk=10)\nprint(\"Top changed SKUs (indices) and avg pct change (promo spike vs baseline):\")\nfor i,p in zip(idx, avg_pct):\n    print(i, f\"{p*100:.2f}%\")\n\n\n# %% [code]\n\n\n# %% [markdown]\n# second set fix\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-11-27T22:23:05.611289Z\",\"iopub.execute_input\":\"2025-11-27T22:23:05.611684Z\",\"iopub.status.idle\":\"2025-11-27T22:23:25.009391Z\",\"shell.execute_reply.started\":\"2025-11-27T22:23:05.611661Z\",\"shell.execute_reply\":\"2025-11-27T22:23:25.008387Z\"}}\nimport os\nimport random\nimport math\nfrom datetime import datetime\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\ntry:\n    from torch_geometric.nn import SAGEConv\n    pyg_available = True\nexcept Exception:\n    pyg_available = False\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# =======================\n# Seeds & device\n# =======================\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(SEED)\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint('Device:', device)\n\n# =======================\n# Paths & params\n# =======================\nSALES_PATH = '/kaggle/input/walmart8/sales_train_validation.csv'\nCAL_PATH = '/kaggle/input/walmart8/calendar.csv'\nPRICES_PATH = '/kaggle/input/walmart8/sell_prices.csv'\n\nSAMPLE_N = 300\nLAST_N_DAYS = 200\nEPOCHS = 200\nN_CRITIC = 5\nLR = 1e-3\nCOHERENCE_WEIGHT = 1.0\nADV_WEIGHT_LOCAL = 0.1\nFORECAST_H = 7\nTOP_K_GRAPH = 5\n\n# =======================\n# Load data\n# =======================\nprint('Loading CSVs...')\nsales = pd.read_csv(SALES_PATH)\ncalendar = pd.read_csv(CAL_PATH)\nprices = pd.read_csv(PRICES_PATH)\nprint('Loaded shapes:', sales.shape, calendar.shape, prices.shape)\n\nday_cols = [c for c in sales.columns if c.startswith('d_')]\nday_cols_sorted = sorted(day_cols, key=lambda x: int(x.split('_')[1]))\nselected_days = day_cols_sorted[-LAST_N_DAYS:]\n\n# Sample subset for speed\nsample_df = sales.sample(n=SAMPLE_N, random_state=SEED).reset_index(drop=True)\nsales_sample = sample_df[['id','item_id','dept_id','cat_id','store_id','state_id'] + selected_days].copy()\n\n# =======================\n# Mappings\n# =======================\ndept_uniques = sales_sample['dept_id'].unique()\ncat_uniques = sales_sample['cat_id'].unique()\ndept_map = {v:i for i,v in enumerate(dept_uniques)}\ncat_map = {v:i for i,v in enumerate(cat_uniques)}\n\nsku_item_ids = sales_sample['item_id'].astype(str).values\nsku2dept = sales_sample['dept_id'].map(dept_map).values\ndept2cat = np.zeros(len(dept_uniques), dtype=int)\nfor d_val, d_idx in dept_map.items():\n    cat_val = sales_sample.loc[sales_sample['dept_id']==d_val, 'cat_id'].iloc[0]\n    dept2cat[d_idx] = cat_map[cat_val]\n\nn_skus = len(sales_sample)\nn_depts = len(dept_uniques)\nn_categories = len(cat_uniques)\nprint('n_skus, n_depts, n_categories =', n_skus, n_depts, n_categories)\n\n# =======================\n# Prices matrix\n# =======================\nprices_clean = prices.groupby(['item_id','wm_yr_wk'], as_index=False)['sell_price'].mean()\nprices_by_item = {}\nfor item_id, g in prices_clean.groupby('item_id'):\n    g = g.sort_values('wm_yr_wk')\n    prices_by_item[str(item_id)] = (g['wm_yr_wk'].values, g['sell_price'].values.astype(float))\n\n# Calendar map\ncal_map = {f\"d_{i+1}\": wk for i, wk in enumerate(calendar['wm_yr_wk'].values)}\n\nselected_wks = [cal_map[f\"d_{int(c.split('_')[1])}\"] for c in selected_days]\nprice_matrix = np.zeros((n_skus, LAST_N_DAYS), dtype=float)\n\nfor i, item in enumerate(sku_item_ids):\n    item_key = str(item)\n    if item_key in prices_by_item:\n        wk_arr, p_arr = prices_by_item[item_key]\n        if len(wk_arr) > 0:\n            idxs = np.searchsorted(wk_arr, selected_wks, side='right') - 1\n            vals = np.array([p_arr[idx] if idx >= 0 else np.nan for idx in idxs], dtype=float)\n            if np.isnan(vals).any():\n                med = np.nanmedian(p_arr)\n                if np.isnan(med): med = 5.0\n                vals[np.isnan(vals)] = med\n        else:\n            vals = np.full(LAST_N_DAYS, 5.0, dtype=float)\n    else:\n        vals = np.full(LAST_N_DAYS, 5.0 + np.random.rand(LAST_N_DAYS).astype(float), dtype=float)\n    price_matrix[i,:] = vals\n\nprice_tensor = torch.tensor(price_matrix, dtype=torch.float32).to(device)\n\n# =======================\n# Promo matrix\n# =======================\nevent_weeks = set(calendar.loc[calendar['event_name_1'].notnull(), 'wm_yr_wk'].unique()) | \\\n              set(calendar.loc[calendar['event_name_2'].notnull(), 'wm_yr_wk'].unique())\npromo_vector = np.array([1.0 if wk in event_weeks else 0.0 for wk in selected_wks], dtype=float)\npromo_matrix = np.tile(promo_vector, (n_skus,1))\npromo_tensor = torch.tensor(promo_matrix, dtype=torch.float32).to(device)\n\n# =======================\n# Sales preprocessing\n# =======================\nsales_values = sales_sample[selected_days].values.astype(float)\nsales_tensor_raw = torch.tensor(sales_values, dtype=torch.float32).to(device)\nsales_log = torch.log1p(sales_tensor_raw)\nsales_mean = sales_log.mean(dim=1, keepdim=True)\nsales_std = sales_log.std(dim=1, keepdim=True) + 1e-6\nsales_norm = (sales_log - sales_mean) / sales_std\n\nsales_cat_log = torch.zeros(n_categories, LAST_N_DAYS, dtype=torch.float32).to(device)\nfor c in range(n_categories):\n    sku_idxs = [i for i in range(n_skus) if dept2cat[sku2dept[i]] == c]\n    if len(sku_idxs) > 0:\n        sales_cat_log[c] = sales_log[sku_idxs].sum(0)\n\ncat_mean = sales_cat_log.mean(dim=1, keepdim=True)\ncat_std  = sales_cat_log.std(dim=1, keepdim=True) + 1e-6\nsales_cat_norm = (sales_cat_log - cat_mean) / cat_std\n\nprice_mean = price_tensor.mean(dim=1, keepdim=True)\nprice_std  = price_tensor.std(dim=1, keepdim=True) + 1e-6\nprice_norm = (price_tensor - price_mean) / price_std\n\nsales_tensor_norm = sales_norm\nsales_cat = sales_cat_norm\nprice_tensor_norm = price_norm\n\n# =======================\n# Hierarchy matrices\n# =======================\nS_sku2dept = torch.zeros(n_depts, n_skus, dtype=torch.float32)\nfor i in range(n_skus):\n    S_sku2dept[sku2dept[i], i] = 1.0\n\nS_dept2cat = torch.zeros(n_categories, n_depts, dtype=torch.float32)\nfor i in range(n_depts):\n    S_dept2cat[dept2cat[i], i] = 1.0\n\nS_sku2cat = S_dept2cat @ S_sku2dept\nS_sku2cat_norm = S_sku2cat / (S_sku2cat.sum(dim=1, keepdim=True) + 1e-6)\nS_sku2cat_norm = S_sku2cat_norm.to(device)\n\n# =======================\n# Similarity Graph\n# =======================\ndef build_similarity_graph(sales_log_input, top_k=TOP_K_GRAPH, min_edges_per_node=1):\n    if isinstance(sales_log_input, torch.Tensor):\n        arr = sales_log_input.detach().cpu().numpy()\n    else:\n        arr = np.asarray(sales_log_input)\n    sim = cosine_similarity(arr)\n    np.fill_diagonal(sim, -np.inf)\n    edges = []\n    n = sim.shape[0]\n    for i in range(n):\n        k = min(top_k, n-1)\n        top_idx = np.argpartition(-sim[i], k)[:k]\n        added = 0\n        for j in top_idx:\n            if sim[i,j] > 0:\n                edges.append([i, int(j)])\n                added += 1\n        if added < min_edges_per_node:\n            for j in range(n):\n                if j == i: continue\n                if sku2dept[i] == sku2dept[j]:\n                    edges.append([i,int(j)])\n                    added += 1\n                    if added >= min_edges_per_node:\n                        break\n    if len(edges) == 0:\n        for i in range(n):\n            for j in range(i+1,n):\n                edges.append([i,j])\n    edge_index = torch.tensor(edges, dtype=torch.long).T.to(device)\n    return edge_index\n\nedge_index = build_similarity_graph(sales_log, top_k=TOP_K_GRAPH)\n\n# =======================\n# Model definitions\n# =======================\nclass GlobalGenerator(nn.Module):\n    def __init__(self, n_categories, hidden=64):\n        super().__init__()\n        self.lstm = nn.LSTM(n_categories, hidden, batch_first=True)\n        self.attn = nn.MultiheadAttention(hidden, num_heads=4, batch_first=True)\n        self.fc = nn.Linear(hidden, n_categories)\n    def forward(self, x):\n        lstm_out,_ = self.lstm(x)\n        attn_out,_ = self.attn(lstm_out,lstm_out,lstm_out)\n        return self.fc(attn_out[:, -1, :])\n\nclass LocalGenerator(nn.Module):\n    def __init__(self, n_skus, global_size, hidden=64, use_pyg=pyg_available):\n        super().__init__()\n        self.fc1 = nn.Linear(global_size + 2, hidden)\n        self.use_pyg = use_pyg\n        if self.use_pyg:\n            self.gnn = SAGEConv(hidden, hidden)\n        else:\n            self.gnn_lin = nn.Linear(hidden, hidden)\n        self.fc2 = nn.Linear(hidden,1)\n        self.n_skus = n_skus\n    def forward(self, global_forecast, price, promo, edge_index):\n        g = global_forecast\n        if g.dim()==2 and g.size(0)==1:\n            g = g.squeeze(0)\n        x = torch.stack([price, promo], dim=1)\n        global_repeat = g.unsqueeze(0).repeat(self.n_skus,1)\n        x = torch.cat([x, global_repeat], dim=1)\n        x = torch.relu(self.fc1(x))\n        if self.use_pyg:\n            x = self.gnn(x, edge_index)\n        else:\n            x = torch.relu(self.gnn_lin(x))\n        return self.fc2(x)\n\nclass Discriminator(nn.Module):\n    def __init__(self, input_size, hidden=64):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(input_size, hidden),\n            nn.ReLU(),\n            nn.Linear(hidden,1)\n        )\n    def forward(self,x):\n        return self.net(x)\n\nclass ReconciliationLayer(nn.Module):\n    def __init__(self,S):\n        super().__init__()\n        self.S = S\n    def forward(self, y_sku):\n        y = y_sku.squeeze(-1) if y_sku.dim()==2 and y_sku.size(1)==1 else y_sku\n        return self.S @ y\n\ndef gradient_penalty(D, real, fake, device=device):\n    batch_size = real.size(0)\n    alpha = torch.rand(batch_size,1, device=device)\n    alpha = alpha.view(batch_size, *([1]*(real.dim()-1))).expand_as(real)\n    interpolated = (alpha*real + (1-alpha)*fake).requires_grad_(True)\n    pred = D(interpolated)\n    grads = torch.autograd.grad(outputs=pred, inputs=interpolated,\n                                grad_outputs=torch.ones_like(pred, device=device),\n                                create_graph=True, retain_graph=True)[0]\n    grad_norm = grads.view(batch_size,-1).norm(2, dim=1)\n    return ((grad_norm - 1.0)**2).mean()\n\n# =======================\n# Instantiate models & optimizers\n# =======================\nGg = GlobalGenerator(n_categories).to(device)\nGl = LocalGenerator(n_skus, n_categories).to(device)\nDg = Discriminator(n_categories).to(device)\nDl = Discriminator(1).to(device)\nrecon_layer = ReconciliationLayer(S_sku2cat_norm)\n\nopt_Gg = optim.Adam(Gg.parameters(), lr=LR)\nopt_Gl = optim.Adam(Gl.parameters(), lr=LR)\nopt_Dg = optim.Adam(Dg.parameters(), lr=LR)\nopt_Dl = optim.Adam(Dl.parameters(), lr=LR)\n\nx_seq_input = sales_cat.T.unsqueeze(0)\ny_real_global = sales_cat[:,-1].unsqueeze(0)\nprice_last = price_tensor_norm[:,-1].to(device)\npromo_last = promo_tensor[:,-1].to(device)\ny_sku_true_norm = sales_tensor_norm[:,-1].unsqueeze(-1)\n\n# =======================\n# Training loop\n# =======================\nprint('Starting training...')\nfor epoch in range(EPOCHS):\n    # --- global discriminator\n    for _ in range(N_CRITIC):\n        y_fake_global = Gg(x_seq_input)\n        D_real = Dg(y_real_global)\n        D_fake = Dg(y_fake_global.detach())\n        gp = gradient_penalty(Dg, y_real_global, y_fake_global.detach())\n        loss_Dg = -torch.mean(D_real) + torch.mean(D_fake) + 10.0*gp\n        opt_Dg.zero_grad(); loss_Dg.backward(); opt_Dg.step()\n    y_fake_global = Gg(x_seq_input)\n    loss_Gg = -torch.mean(Dg(y_fake_global))\n    opt_Gg.zero_grad(); loss_Gg.backward(retain_graph=True); opt_Gg.step()\n\n    # --- local generator & discriminator\n    y_fake_global_local = y_fake_global.detach()\n    y_fake_local_norm = Gl(y_fake_global_local, price_last, promo_last, edge_index)\n    for _ in range(N_CRITIC):\n        D_real_local = Dl(y_sku_true_norm)\n        D_fake_local = Dl(y_fake_local_norm.detach())\n        gp_local = gradient_penalty(Dl, y_sku_true_norm, y_fake_local_norm.detach())\n        loss_Dl = -torch.mean(D_real_local) + torch.mean(D_fake_local) + 10.0*gp_local\n        opt_Dl.zero_grad(); loss_Dl.backward(); opt_Dl.step()\n\n    adv_loss_local = -torch.mean(Dl(y_fake_local_norm))\n    mse_local = nn.MSELoss()(y_fake_local_norm.squeeze(-1), y_sku_true_norm.squeeze(-1))\n    y_recon_norm = recon_layer(y_fake_local_norm.squeeze(-1))\n    y_fake_global_for_coh = y_fake_global_local.squeeze(0)\n    loss_coh = nn.MSELoss()(y_recon_norm, y_fake_global_for_coh)\n    loss_Gl = ADV_WEIGHT_LOCAL*adv_loss_local + mse_local + COHERENCE_WEIGHT*loss_coh\n    opt_Gl.zero_grad(); loss_Gl.backward(); opt_Gl.step()\n\n    if (epoch+1)%10==0 or epoch==0:\n        with torch.no_grad():\n            rmse_sku = torch.sqrt(((y_sku_true_norm.squeeze(-1)-y_fake_local_norm.squeeze(-1))**2).mean()).item()\n            coh_val = loss_coh.item()\n            print(f\"Epoch {epoch+1}/{EPOCHS} | Dg {loss_Dg.item():.4f} | Dl {loss_Dl.item():.4f} | Gg {loss_Gg.item():.4f} | Gl {loss_Gl.item():.4f} | rmse_sku_norm {rmse_sku:.4f} | coh {coh_val:.4f}\")\n\nprint('Training finished.')\n\n# =======================\n# Forecasting\n# =======================\nH = FORECAST_H\nx_sku = sales_tensor_norm.clone()\nx_cat = sales_cat.clone()\nprice_hist = price_tensor_norm.clone()\npromo_hist = promo_tensor.clone()\nfuture_local = []\nfuture_global = []\n\nwith torch.no_grad():\n    for step in range(H):\n        global_in = x_cat.T.unsqueeze(0)\n        g_pred = Gg(global_in).squeeze(0)\n        future_global.append(g_pred)\n        price_last_step = price_hist[:,-1]\n        promo_last_step = promo_hist[:,-1]\n        local_pred = Gl(g_pred, price_last_step, promo_last_step, edge_index).squeeze(-1)\n        future_local.append(local_pred)\n        x_sku = torch.cat([x_sku[:,1:], local_pred.unsqueeze(1)], dim=1)\n        next_cat = S_sku2cat_norm @ local_pred\n        x_cat = torch.cat([x_cat[:,1:], next_cat.unsqueeze(1)], dim=1)\n        price_hist = torch.cat([price_hist[:,1:], price_last_step.unsqueeze(1)], dim=1)\n        promo_hist = torch.cat([promo_hist[:,1:], promo_last_step.unsqueeze(1)], dim=1)\n\nfuture_local_norm = torch.stack(future_local, dim=1)\nfuture_local_raw = torch.expm1(future_local_norm * sales_std + sales_mean)\nprint('Future shapes:', future_local_norm.shape, future_local_raw.shape)\n\nwith torch.no_grad():\n    y_sku_true_raw = sales_tensor_raw[:,-1]\n    y_sku_pred_raw = future_local_raw[:,0]\n    rmse_raw = torch.sqrt(((y_sku_true_raw - y_sku_pred_raw)**2).mean()).item()\n    mae_raw = torch.mean(torch.abs(y_sku_true_raw - y_sku_pred_raw)).item()\n    print(f'RAW-scale SKU RMSE: {rmse_raw:.4f} | MAE: {mae_raw:.4f}')\n\n# =======================\n# Plot 5 SKUs\n# =======================\nsample_idx = np.random.choice(n_skus, size=5, replace=False)\nfig, axs = plt.subplots(5,1, figsize=(8,12))\nfor i, idx in enumerate(sample_idx):\n    axs[i].plot(np.arange(LAST_N_DAYS), sales_tensor_raw[idx].cpu().numpy(), label='history')\n    axs[i].plot(np.arange(LAST_N_DAYS, LAST_N_DAYS+H), future_local_raw[idx].cpu().numpy(), label='forecast')\n    axs[i].legend()\n    axs[i].set_title(f'SKU idx {idx}')\nplt.tight_layout()\nplt.show()\n\n# =======================\n# Save models\n# =======================\nos.makedirs('/kaggle/working/models', exist_ok=True)\ntorch.save(Gg.state_dict(), '/kaggle/working/models/Gg.pth')\ntorch.save(Gl.state_dict(), '/kaggle/working/models/Gl.pth')\nprint('Models saved to /kaggle/working/models')\n\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-11-27T22:30:12.309464Z\",\"iopub.execute_input\":\"2025-11-27T22:30:12.309825Z\",\"iopub.status.idle\":\"2025-11-27T22:30:21.029322Z\",\"shell.execute_reply.started\":\"2025-11-27T22:30:12.309799Z\",\"shell.execute_reply\":\"2025-11-27T22:30:21.028053Z\"}}\n\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-11-27T23:20:49.265165Z\",\"iopub.execute_input\":\"2025-11-27T23:20:49.265561Z\",\"iopub.status.idle\":\"2025-11-27T23:21:01.461691Z\",\"shell.execute_reply.started\":\"2025-11-27T23:20:49.265538Z\",\"shell.execute_reply\":\"2025-11-27T23:21:01.460719Z\"}}\n# =======================\n# Imports & setup\n# =======================\nimport os, random, numpy as np, pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport matplotlib.pyplot as plt\n\ntry:\n    from torch_geometric.nn import SAGEConv\n    pyg_available = True\nexcept:\n    pyg_available = False\n\n# =======================\n# Seeds & device\n# =======================\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\nif torch.cuda.is_available(): torch.cuda.manual_seed_all(SEED)\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint('Device:', device)\n\n# =======================\n# Paths & hyperparams\n# =======================\nSALES_PATH = '/kaggle/input/walmart8/sales_train_validation.csv'\nCAL_PATH = '/kaggle/input/walmart8/calendar.csv'\nPRICES_PATH = '/kaggle/input/walmart8/sell_prices.csv'\n\nSAMPLE_N = 300\nLAST_N_DAYS = 200\nEPOCHS = 50\nN_CRITIC = 5\nLR = 1e-3\nCOHERENCE_WEIGHT = 1.0\nADV_WEIGHT_LOCAL = 0.1\nFORECAST_H = 7\nTOP_K_GRAPH = 5\n\n# =======================\n# Load data\n# =======================\nprint(\"Loading CSVs...\")\nsales = pd.read_csv(SALES_PATH)\ncalendar = pd.read_csv(CAL_PATH)\nprices = pd.read_csv(PRICES_PATH)\nprint('Shapes:', sales.shape, calendar.shape, prices.shape)\n\nday_cols = [c for c in sales.columns if c.startswith('d_')]\nday_cols_sorted = sorted(day_cols, key=lambda x:int(x.split('_')[1]))\nselected_days = day_cols_sorted[-LAST_N_DAYS:]\n\n# Sample subset\nsample_df = sales.sample(n=SAMPLE_N, random_state=SEED).reset_index(drop=True)\nsales_sample = sample_df[['id','item_id','dept_id','cat_id','store_id','state_id'] + selected_days].copy()\n\n# =======================\n# Mappings\n# =======================\ndept_uniques = sales_sample['dept_id'].unique()\ncat_uniques = sales_sample['cat_id'].unique()\ndept_map = {v:i for i,v in enumerate(dept_uniques)}\ncat_map = {v:i for i,v in enumerate(cat_uniques)}\n\nsku_item_ids = sales_sample['item_id'].astype(str).values\nsku2dept = sales_sample['dept_id'].map(dept_map).values\ndept2cat = np.zeros(len(dept_uniques), dtype=int)\nfor d_val,d_idx in dept_map.items():\n    cat_val = sales_sample.loc[sales_sample['dept_id']==d_val,'cat_id'].iloc[0]\n    dept2cat[d_idx] = cat_map[cat_val]\n\nn_skus = len(sales_sample)\nn_depts = len(dept_uniques)\nn_categories = len(cat_uniques)\nprint('n_skus, n_depts, n_categories =', n_skus, n_depts, n_categories)\n\n# =======================\n# Prices matrix\n# =======================\nprices_clean = prices.groupby(['item_id','wm_yr_wk'], as_index=False)['sell_price'].mean()\nprices_by_item = {}\nfor item_id, g in prices_clean.groupby('item_id'):\n    g = g.sort_values('wm_yr_wk')\n    prices_by_item[str(item_id)] = (g['wm_yr_wk'].values, g['sell_price'].values.astype(float))\n\ncal_map = {f\"d_{i+1}\": wk for i,wk in enumerate(calendar['wm_yr_wk'].values)}\nselected_wks = [cal_map[c] for c in selected_days]\n\nprice_matrix = np.zeros((n_skus, LAST_N_DAYS), dtype=float)\nfor i, item in enumerate(sku_item_ids):\n    item_key = str(item)\n    if item_key in prices_by_item:\n        wk_arr, p_arr = prices_by_item[item_key]\n        idxs = np.searchsorted(wk_arr, selected_wks, side='right') - 1\n        vals = np.array([p_arr[idx] if idx>=0 else np.nan for idx in idxs], dtype=float)\n        if np.isnan(vals).any(): vals[np.isnan(vals)] = np.nanmedian(p_arr)\n    else:\n        vals = np.full(LAST_N_DAYS, 5.0, dtype=float)\n    price_matrix[i,:] = vals\nprice_tensor = torch.tensor(price_matrix, dtype=torch.float32).to(device)\n\n# =======================\n# Promo matrix\n# =======================\nevent_weeks = set(calendar.loc[calendar['event_name_1'].notnull(),'wm_yr_wk'].unique()) | \\\n              set(calendar.loc[calendar['event_name_2'].notnull(),'wm_yr_wk'].unique())\npromo_vector = np.array([1.0 if wk in event_weeks else 0.0 for wk in selected_wks], dtype=float)\npromo_matrix = np.tile(promo_vector, (n_skus,1))\npromo_tensor = torch.tensor(promo_matrix, dtype=torch.float32).to(device)\n\n# =======================\n# Sales preprocessing\n# =======================\nsales_values = sales_sample[selected_days].values.astype(float)\nsales_tensor_raw = torch.tensor(sales_values, dtype=torch.float32).to(device)\nsales_log = torch.log1p(sales_tensor_raw)\nsales_mean = sales_log.mean(dim=1, keepdim=True)\nsales_std = sales_log.std(dim=1, keepdim=True) + 1e-6\nsales_norm = (sales_log - sales_mean)/sales_std\n\n# Category level aggregation\nsales_cat_log = torch.zeros(n_categories, LAST_N_DAYS, dtype=torch.float32).to(device)\nfor c in range(n_categories):\n    sku_idxs = [i for i in range(n_skus) if dept2cat[sku2dept[i]]==c]\n    if len(sku_idxs)>0:\n        sales_cat_log[c] = sales_log[sku_idxs].sum(0)\ncat_mean = sales_cat_log.mean(dim=1, keepdim=True)\ncat_std = sales_cat_log.std(dim=1, keepdim=True)+1e-6\nsales_cat_norm = (sales_cat_log-cat_mean)/cat_std\n\n# Normalized tensors\nsales_tensor_norm = sales_norm\nsales_cat = sales_cat_norm\nprice_tensor_norm = (price_tensor-price_tensor.mean(dim=1,keepdim=True)) / (price_tensor.std(dim=1,keepdim=True)+1e-6)\n\n# =======================\n# Hierarchy matrices\n# =======================\nS_sku2dept = torch.zeros(n_depts, n_skus, dtype=torch.float32).to(device)\nfor i in range(n_skus): S_sku2dept[sku2dept[i], i] = 1.0\n\nS_dept2cat = torch.zeros(n_categories, n_depts, dtype=torch.float32).to(device)\nfor i in range(n_depts): S_dept2cat[dept2cat[i], i] = 1.0\n\nS_sku2cat = S_dept2cat @ S_sku2dept\nS_sku2cat_norm = S_sku2cat / (S_sku2cat.sum(dim=1, keepdim=True)+1e-6)\n\n# =======================\n# Day-of-week one-hot\n# =======================\ndow_raw = np.array([calendar.loc[calendar['d']==d,'wday'].values[0] for d in selected_days])\ndow_raw = np.clip(dow_raw,0,6)\ndow_onehot = np.eye(7)[dow_raw]\ndow_tensor = torch.tensor(np.tile(dow_onehot,(n_skus,1,1)),dtype=torch.float32).to(device)\n\n# =======================\n# Similarity graph\n# =======================\ndef build_similarity_graph(sales_log_input, top_k=TOP_K_GRAPH):\n    arr = sales_log_input.detach().cpu().numpy()\n    sim = cosine_similarity(arr)\n    np.fill_diagonal(sim, -np.inf)\n    edges = []\n    n = sim.shape[0]\n    for i in range(n):\n        k = min(top_k, n-1)\n        top_idx = np.argpartition(-sim[i], k)[:k]\n        for j in top_idx:\n            if sim[i,j]>0: edges.append([i,int(j)])\n    if len(edges)==0:\n        for i in range(n):\n            for j in range(i+1,n): edges.append([i,j])\n    return torch.tensor(edges,dtype=torch.long).T.to(device)\nedge_index = build_similarity_graph(sales_log)\n\n# =======================\n# Model definitions\n# =======================\nclass GlobalGenerator(nn.Module):\n    def __init__(self, n_categories, hidden=64):\n        super().__init__()\n        self.lstm = nn.LSTM(n_categories, hidden, batch_first=True)\n        self.attn = nn.MultiheadAttention(hidden, num_heads=4, batch_first=True)\n        self.fc = nn.Linear(hidden,n_categories)\n    def forward(self,x):\n        lstm_out,_ = self.lstm(x)\n        attn_out,_ = self.attn(lstm_out,lstm_out,lstm_out)\n        return self.fc(attn_out[:,-1,:])\n\nclass LocalGenerator(nn.Module):\n    def __init__(self, n_skus, global_size, dow_size=7, hidden=64, use_pyg=pyg_available):\n        super().__init__()\n        self.fc1 = nn.Linear(global_size+2+dow_size, hidden)\n        self.use_pyg = use_pyg\n        if self.use_pyg:\n            self.gnn = SAGEConv(hidden,hidden)\n        else:\n            self.gnn_lin = nn.Linear(hidden,hidden)\n        self.fc2 = nn.Linear(hidden,1)\n        self.n_skus = n_skus\n    def forward(self, global_forecast, price, promo, dow, edge_index):\n        g = global_forecast\n        if g.dim()==1: g = g.unsqueeze(0)\n        g_repeat = g.repeat(self.n_skus,1)\n        price = price.unsqueeze(1)\n        promo = promo.unsqueeze(1)\n        x = torch.cat([price,promo,dow,g_repeat],dim=1)\n        x = torch.relu(self.fc1(x))\n        if self.use_pyg: x = self.gnn(x,edge_index)\n        else: x = torch.relu(self.gnn_lin(x))\n        return self.fc2(x)\n\nclass Discriminator(nn.Module):\n    def __init__(self,input_size,hidden=64):\n        super().__init__()\n        self.net = nn.Sequential(nn.Linear(input_size,hidden),nn.ReLU(),nn.Linear(hidden,1))\n    def forward(self,x):\n        return self.net(x)\n\nclass ReconciliationLayer(nn.Module):\n    def __init__(self,S):\n        super().__init__()\n        self.S = S\n    def forward(self,y_sku):\n        y = y_sku.squeeze(-1) if y_sku.dim()==2 and y_sku.size(1)==1 else y_sku\n        return self.S @ y\n\ndef gradient_penalty(D, real, fake, device=device):\n    batch_size = real.size(0)\n    alpha = torch.rand(batch_size,1,device=device)\n    alpha = alpha.view(batch_size, *([1]*(real.dim()-1))).expand_as(real)\n    interpolated = (alpha*real+(1-alpha)*fake).requires_grad_(True)\n    pred = D(interpolated)\n    grads = torch.autograd.grad(outputs=pred,inputs=interpolated,\n                                grad_outputs=torch.ones_like(pred,device=device),\n                                create_graph=True, retain_graph=True)[0]\n    grad_norm = grads.view(batch_size,-1).norm(2,dim=1)\n    return ((grad_norm-1.0)**2).mean()\n\n# =======================\n# Instantiate models & optimizers\n# =======================\nGg = GlobalGenerator(n_categories).to(device)\nGl = LocalGenerator(n_skus,n_categories,dow_size=7).to(device)\nDg = Discriminator(n_categories).to(device)\nDl = Discriminator(1).to(device)\nrecon_layer = ReconciliationLayer(S_sku2cat_norm)\n\nopt_Gg = optim.Adam(Gg.parameters(),lr=LR)\nopt_Gl = optim.Adam(Gl.parameters(),lr=LR)\nopt_Dg = optim.Adam(Dg.parameters(),lr=LR)\nopt_Dl = optim.Adam(Dl.parameters(),lr=LR)\n\nx_seq_input = sales_cat.T.unsqueeze(0)\ny_real_global = sales_cat[:,-1].unsqueeze(0)\nprice_last = price_tensor_norm[:,-1].to(device)\npromo_last = promo_tensor[:,-1].to(device)\ndow_last = dow_tensor[:,-1,:].to(device)\ny_sku_true_norm = sales_tensor_norm[:,-1].unsqueeze(-1)\n\n# =======================\n# Training loop\n# =======================\nprint(\"Starting training...\")\nfor epoch in range(EPOCHS):\n    # --- Global discriminator\n    for _ in range(N_CRITIC):\n        y_fake_global = Gg(x_seq_input)\n        D_real = Dg(y_real_global)\n        D_fake = Dg(y_fake_global.detach())\n        gp = gradient_penalty(Dg,y_real_global,y_fake_global.detach())\n        loss_Dg = -torch.mean(D_real)+torch.mean(D_fake)+10*gp\n        opt_Dg.zero_grad(); loss_Dg.backward(); opt_Dg.step()\n    y_fake_global = Gg(x_seq_input)\n    loss_Gg = -torch.mean(Dg(y_fake_global))\n    opt_Gg.zero_grad(); loss_Gg.backward(retain_graph=True); opt_Gg.step()\n\n    # --- Local generator & discriminator\n    y_fake_global_local = y_fake_global.detach()\n    y_fake_local_norm = Gl(y_fake_global_local, price_last, promo_last, dow_last, edge_index)\n    for _ in range(N_CRITIC):\n        D_real_local = Dl(y_sku_true_norm)\n        D_fake_local = Dl(y_fake_local_norm.detach())\n        gp_local = gradient_penalty(Dl,y_sku_true_norm,y_fake_local_norm.detach())\n        loss_Dl = -torch.mean(D_real_local)+torch.mean(D_fake_local)+10*gp_local\n        opt_Dl.zero_grad(); loss_Dl.backward(); opt_Dl.step()\n\n    adv_loss_local = -torch.mean(Dl(y_fake_local_norm))\n    mse_local = nn.MSELoss()(y_fake_local_norm.squeeze(-1), y_sku_true_norm.squeeze(-1))\n    y_recon_norm = recon_layer(y_fake_local_norm.squeeze(-1))\n    loss_coh = nn.MSELoss()(y_recon_norm, y_fake_global_local.squeeze(0))\n    loss_Gl = ADV_WEIGHT_LOCAL*adv_loss_local + mse_local + COHERENCE_WEIGHT*loss_coh\n    opt_Gl.zero_grad(); loss_Gl.backward(); opt_Gl.step()\n\n    if (epoch+1)%10==0 or epoch==0:\n        with torch.no_grad():\n            rmse_sku = torch.sqrt(((y_sku_true_norm.squeeze(-1)-y_fake_local_norm.squeeze(-1))**2).mean()).item()\n            print(f\"Epoch {epoch+1}/{EPOCHS} | Dg {loss_Dg.item():.4f} | Dl {loss_Dl.item():.4f} | Gg {loss_Gg.item():.4f} | Gl {loss_Gl.item():.4f} | RMSE {rmse_sku:.4f} | Coh {loss_coh.item():.4f}\")\n\nprint(\"Training finished.\")\n\n# =======================\n# Forecasting\n# =======================\nH = FORECAST_H\nx_sku = sales_tensor_norm.clone()\nx_cat = sales_cat.clone()\nprice_hist = price_tensor_norm.clone()\npromo_hist = promo_tensor.clone()\ndow_hist = dow_tensor.clone()\nfuture_local = []\nfuture_global = []\n\nwith torch.no_grad():\n    for step in range(H):\n        global_in = x_cat.T.unsqueeze(0)\n        g_pred = Gg(global_in).squeeze(0)\n        future_global.append(g_pred)\n        price_last_step = price_hist[:,-1]\n        promo_last_step = promo_hist[:,-1]\n        dow_last_step = dow_hist[:,-1,:]\n        local_pred = Gl(g_pred, price_last_step, promo_last_step, dow_last_step, edge_index).squeeze(-1)\n        future_local.append(local_pred)\n        x_sku = torch.cat([x_sku[:,1:], local_pred.unsqueeze(1)], dim=1)\n        next_cat = S_sku2cat_norm @ local_pred\n        x_cat = torch.cat([x_cat[:,1:], next_cat.unsqueeze(1)], dim=1)\n        price_hist = torch.cat([price_hist[:,1:], price_last_step.unsqueeze(1)], dim=1)\n        promo_hist = torch.cat([promo_hist[:,1:], promo_last_step.unsqueeze(1)], dim=1)\n        dow_hist = torch.cat([dow_hist[:,1:], dow_last_step.unsqueeze(1)], dim=1)\n\nfuture_local_norm = torch.stack(future_local,dim=1)\nfuture_local_raw = torch.expm1(future_local_norm*sales_std+sales_mean)\nprint('Forecast shape:', future_local_raw.shape)\n\n# =======================\n# Evaluation functions\n# =======================\ndef compute_wrmsse(y_true, y_pred, S_matrix, scale_vec):\n    y_true_agg = S_matrix @ y_true\n    y_pred_agg = S_matrix @ y_pred\n    scale_agg = (S_matrix @ scale_vec[:,None])+1e-6\n    mse = ((y_true_agg - y_pred_agg)**2)/scale_agg\n    return torch.sqrt(mse.mean()).item()\n\ndef gan_quality_metrics(real, fake):\n    real_flat = real.detach().cpu().flatten()\n    fake_flat = fake.detach().cpu().flatten()\n    hist_real = np.histogram(real_flat, bins=50, density=True)[0]+1e-8\n    hist_fake = np.histogram(fake_flat, bins=50, density=True)[0]+1e-8\n    hist_real /= hist_real.sum(); hist_fake/=hist_fake.sum()\n    kl = np.sum(hist_real*np.log(hist_real/hist_fake))\n    wd = np.sum(np.abs(np.cumsum(hist_real)-np.cumsum(hist_fake)))\n    return kl, wd\n\ndef pinball_loss(y_true,y_pred,tau=0.5):\n    diff = y_true-y_pred\n    loss = torch.max(tau*diff,(tau-1)*diff)\n    return loss.mean().item()\n\n# =======================\n# Forecast evaluation\n# =======================\nwith torch.no_grad():\n    y_true_forecast = sales_tensor_raw[:,-FORECAST_H:]\n    y_pred_forecast = future_local_raw\n    scale_vec = sales_tensor_raw.var(dim=1)+1e-6\n\n    wrmsse_sku = compute_wrmsse(y_true_forecast, y_pred_forecast, torch.eye(n_skus).to(device), scale_vec)\n    wrmsse_cat = compute_wrmsse(y_true_forecast, y_pred_forecast, S_sku2cat_norm, scale_vec)\n    kl_div, wd = gan_quality_metrics(y_sku_true_norm, y_fake_local_norm)\n    pinball = pinball_loss(y_true_forecast, y_pred_forecast, tau=0.5)\n\n    print(f\"WRMSSE SKU: {wrmsse_sku:.4f} | WRMSSE Cat: {wrmsse_cat:.4f}\")\n    print(f\"GAN KL: {kl_div:.4f} | Wasserstein: {wd:.4f}\")\n    print(f\"Pinball Loss (tau=0.5): {pinball:.4f}\")\n\n# =======================\n# Plot 5 random SKUs\n# =======================\nsample_idx = np.random.choice(n_skus,5,replace=False)\nfig, axs = plt.subplots(5,1,figsize=(10,12))\nfor i, idx in enumerate(sample_idx):\n    axs[i].plot(np.arange(LAST_N_DAYS), sales_tensor_raw[idx].cpu().numpy(), label='History')\n    axs[i].plot(np.arange(LAST_N_DAYS,LAST_N_DAYS+FORECAST_H), future_local_raw[idx].cpu().numpy(), label='Forecast')\n    axs[i].legend(); axs[i].set_title(f'SKU {idx}')\nplt.tight_layout(); plt.show()\n\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-11-27T23:31:29.79645Z\",\"iopub.execute_input\":\"2025-11-27T23:31:29.796881Z\",\"iopub.status.idle\":\"2025-11-27T23:31:42.246196Z\",\"shell.execute_reply.started\":\"2025-11-27T23:31:29.796852Z\",\"shell.execute_reply\":\"2025-11-27T23:31:42.244943Z\"}}\n# =======================\n# Imports & setup\n# =======================\nimport os, random, numpy as np, pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport matplotlib.pyplot as plt\n\ntry:\n    from torch_geometric.nn import SAGEConv\n    pyg_available = True\nexcept:\n    pyg_available = False\n\n# =======================\n# Seeds & device\n# =======================\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\nif torch.cuda.is_available(): torch.cuda.manual_seed_all(SEED)\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint('Device:', device)\n\n# =======================\n# Paths & hyperparams\n# =======================\nSALES_PATH = '/kaggle/input/walmart8/sales_train_validation.csv'\nCAL_PATH = '/kaggle/input/walmart8/calendar.csv'\nPRICES_PATH = '/kaggle/input/walmart8/sell_prices.csv'\n\nSAMPLE_N = 300\nLAST_N_DAYS = 200\nEPOCHS = 50\nN_CRITIC = 5\nLR = 1e-3\nCOHERENCE_WEIGHT = 1.0\nADV_WEIGHT_LOCAL = 0.1\nCAUSAL_WEIGHT = 0.1\nFORECAST_H = 7\nTOP_K_GRAPH = 5\n\n# =======================\n# Load data\n# =======================\nprint(\"Loading CSVs...\")\nsales = pd.read_csv(SALES_PATH)\ncalendar = pd.read_csv(CAL_PATH)\nprices = pd.read_csv(PRICES_PATH)\nprint('Shapes:', sales.shape, calendar.shape, prices.shape)\n\nday_cols = [c for c in sales.columns if c.startswith('d_')]\nday_cols_sorted = sorted(day_cols, key=lambda x:int(x.split('_')[1]))\nselected_days = day_cols_sorted[-LAST_N_DAYS:]\n\n# Sample subset\nsample_df = sales.sample(n=SAMPLE_N, random_state=SEED).reset_index(drop=True)\nsales_sample = sample_df[['id','item_id','dept_id','cat_id','store_id','state_id'] + selected_days].copy()\n\n# =======================\n# Mappings\n# =======================\ndept_uniques = sales_sample['dept_id'].unique()\ncat_uniques = sales_sample['cat_id'].unique()\ndept_map = {v:i for i,v in enumerate(dept_uniques)}\ncat_map = {v:i for i,v in enumerate(cat_uniques)}\n\nsku_item_ids = sales_sample['item_id'].astype(str).values\nsku2dept = sales_sample['dept_id'].map(dept_map).values\ndept2cat = np.zeros(len(dept_uniques), dtype=int)\nfor d_val,d_idx in dept_map.items():\n    cat_val = sales_sample.loc[sales_sample['dept_id']==d_val,'cat_id'].iloc[0]\n    dept2cat[d_idx] = cat_map[cat_val]\n\nn_skus = len(sales_sample)\nn_depts = len(dept_uniques)\nn_categories = len(cat_uniques)\nprint('n_skus, n_depts, n_categories =', n_skus, n_depts, n_categories)\n\n# =======================\n# Prices matrix\n# =======================\nprices_clean = prices.groupby(['item_id','wm_yr_wk'], as_index=False)['sell_price'].mean()\nprices_by_item = {}\nfor item_id, g in prices_clean.groupby('item_id'):\n    g = g.sort_values('wm_yr_wk')\n    prices_by_item[str(item_id)] = (g['wm_yr_wk'].values, g['sell_price'].values.astype(float))\n\ncal_map = {f\"d_{i+1}\": wk for i,wk in enumerate(calendar['wm_yr_wk'].values)}\nselected_wks = [cal_map[c] for c in selected_days]\n\nprice_matrix = np.zeros((n_skus, LAST_N_DAYS), dtype=float)\nfor i, item in enumerate(sku_item_ids):\n    item_key = str(item)\n    if item_key in prices_by_item:\n        wk_arr, p_arr = prices_by_item[item_key]\n        idxs = np.searchsorted(wk_arr, selected_wks, side='right') - 1\n        vals = np.array([p_arr[idx] if idx>=0 else np.nan for idx in idxs], dtype=float)\n        if np.isnan(vals).any(): vals[np.isnan(vals)] = np.nanmedian(p_arr)\n    else:\n        vals = np.full(LAST_N_DAYS, 5.0, dtype=float)\n    price_matrix[i,:] = vals\nprice_tensor = torch.tensor(price_matrix, dtype=torch.float32).to(device)\n\n# =======================\n# Promo matrix\n# =======================\nevent_weeks = set(calendar.loc[calendar['event_name_1'].notnull(),'wm_yr_wk'].unique()) | \\\n              set(calendar.loc[calendar['event_name_2'].notnull(),'wm_yr_wk'].unique())\npromo_vector = np.array([1.0 if wk in event_weeks else 0.0 for wk in selected_wks], dtype=float)\npromo_matrix = np.tile(promo_vector, (n_skus,1))\npromo_tensor = torch.tensor(promo_matrix, dtype=torch.float32).to(device)\n\n# =======================\n# Sales preprocessing\n# =======================\nsales_values = sales_sample[selected_days].values.astype(float)\nsales_tensor_raw = torch.tensor(sales_values, dtype=torch.float32).to(device)\nsales_log = torch.log1p(sales_tensor_raw)\nsales_mean = sales_log.mean(dim=1, keepdim=True)\nsales_std = sales_log.std(dim=1, keepdim=True) + 1e-6\nsales_norm = (sales_log - sales_mean)/sales_std\n\n# Category level aggregation\nsales_cat_log = torch.zeros(n_categories, LAST_N_DAYS, dtype=torch.float32).to(device)\nfor c in range(n_categories):\n    sku_idxs = [i for i in range(n_skus) if dept2cat[sku2dept[i]]==c]\n    if len(sku_idxs)>0:\n        sales_cat_log[c] = sales_log[sku_idxs].sum(0)\ncat_mean = sales_cat_log.mean(dim=1, keepdim=True)\ncat_std = sales_cat_log.std(dim=1, keepdim=True)+1e-6\nsales_cat_norm = (sales_cat_log-cat_mean)/cat_std\n\n# Normalized tensors\nsales_tensor_norm = sales_norm\nsales_cat = sales_cat_norm\nprice_tensor_norm = (price_tensor-price_tensor.mean(dim=1,keepdim=True)) / (price_tensor.std(dim=1,keepdim=True)+1e-6)\n\n# =======================\n# Hierarchy matrices\n# =======================\nS_sku2dept = torch.zeros(n_depts, n_skus, dtype=torch.float32).to(device)\nfor i in range(n_skus): S_sku2dept[sku2dept[i], i] = 1.0\n\nS_dept2cat = torch.zeros(n_categories, n_depts, dtype=torch.float32).to(device)\nfor i in range(n_depts): S_dept2cat[dept2cat[i], i] = 1.0\n\nS_sku2cat = S_dept2cat @ S_sku2dept\nS_sku2cat_norm = S_sku2cat / (S_sku2cat.sum(dim=1, keepdim=True)+1e-6)\n\n# =======================\n# Day-of-week one-hot\n# =======================\ndow_raw = np.array([calendar.loc[calendar['d']==d,'wday'].values[0] for d in selected_days])\ndow_raw = np.clip(dow_raw,0,6)\ndow_onehot = np.eye(7)[dow_raw]\ndow_tensor = torch.tensor(np.tile(dow_onehot,(n_skus,1,1)),dtype=torch.float32).to(device)\n\n# =======================\n# Similarity graph\n# =======================\ndef build_similarity_graph(sales_log_input, top_k=TOP_K_GRAPH):\n    arr = sales_log_input.detach().cpu().numpy()\n    sim = cosine_similarity(arr)\n    np.fill_diagonal(sim, -np.inf)\n    edges = []\n    n = sim.shape[0]\n    for i in range(n):\n        k = min(top_k, n-1)\n        top_idx = np.argpartition(-sim[i], k)[:k]\n        for j in top_idx:\n            if sim[i,j]>0: edges.append([i,int(j)])\n    if len(edges)==0:\n        for i in range(n):\n            for j in range(i+1,n): edges.append([i,j])\n    return torch.tensor(edges,dtype=torch.long).T.to(device)\nedge_index = build_similarity_graph(sales_log)\n\n# =======================\n# Model definitions\n# =======================\nclass GlobalGenerator(nn.Module):\n    def __init__(self, n_categories, hidden=64):\n        super().__init__()\n        self.lstm = nn.LSTM(n_categories, hidden, batch_first=True)\n        self.attn = nn.MultiheadAttention(hidden, num_heads=4, batch_first=True)\n        self.fc = nn.Linear(hidden,n_categories)\n    def forward(self,x):\n        lstm_out,_ = self.lstm(x)\n        attn_out,_ = self.attn(lstm_out,lstm_out,lstm_out)\n        return self.fc(attn_out[:,-1,:])\n\nclass LocalGenerator(nn.Module):\n    def __init__(self, n_skus, global_size, dow_size=7, hidden=64, use_pyg=pyg_available):\n        super().__init__()\n        self.fc1 = nn.Linear(global_size+2+dow_size, hidden)\n        self.use_pyg = use_pyg\n        if self.use_pyg:\n            self.gnn = SAGEConv(hidden,hidden)\n        else:\n            self.gnn_lin = nn.Linear(hidden,hidden)\n        self.fc2 = nn.Linear(hidden,1)\n        self.n_skus = n_skus\n    def forward(self, global_forecast, price, promo, dow, edge_index):\n        g = global_forecast\n        if g.dim()==1: g = g.unsqueeze(0)\n        g_repeat = g.repeat(self.n_skus,1)\n        price = price.unsqueeze(1)\n        promo = promo.unsqueeze(1)\n        x = torch.cat([price,promo,dow,g_repeat],dim=1)\n        x = torch.relu(self.fc1(x))\n        if self.use_pyg: x = self.gnn(x,edge_index)\n        else: x = torch.relu(self.gnn_lin(x))\n        return self.fc2(x)\n\nclass Discriminator(nn.Module):\n    def __init__(self,input_size,hidden=64):\n        super().__init__()\n        self.net = nn.Sequential(nn.Linear(input_size,hidden),nn.ReLU(),nn.Linear(hidden,1))\n    def forward(self,x):\n        return self.net(x)\n\nclass ReconciliationLayer(nn.Module):\n    def __init__(self,S):\n        super().__init__()\n        self.S = S\n    def forward(self,y_sku):\n        y = y_sku.squeeze(-1) if y_sku.dim()==2 and y_sku.size(1)==1 else y_sku\n        return self.S @ y\n\ndef gradient_penalty(D, real, fake, device=device):\n    batch_size = real.size(0)\n    alpha = torch.rand(batch_size,1,device=device)\n    alpha = alpha.view(batch_size, *([1]*(real.dim()-1))).expand_as(real)\n    interpolated = (alpha*real+(1-alpha)*fake).requires_grad_(True)\n    pred = D(interpolated)\n    grads = torch.autograd.grad(outputs=pred,inputs=interpolated,\n                                grad_outputs=torch.ones_like(pred,device=device),\n                                create_graph=True, retain_graph=True)[0]\n    grad_norm = grads.view(batch_size,-1).norm(2,dim=1)\n    return ((grad_norm-1.0)**2).mean()\n\n# =======================\n# Causal loss\n# =======================\ndef causal_loss(y_pred, price, promo, tau=1.0):\n    corr_price = torch.mean((y_pred - y_pred.mean()) * (price - price.mean()))\n    corr_promo = torch.mean((y_pred - y_pred.mean()) * (promo - promo.mean()))\n    loss = tau * (torch.relu(corr_price) + torch.relu(-corr_promo))\n    return loss\n\n# =======================\n# Instantiate models & optimizers\n# =======================\nGg = GlobalGenerator(n_categories).to(device)\nGl = LocalGenerator(n_skus,n_categories,dow_size=7).to(device)\nDg = Discriminator(n_categories).to(device)\nDl = Discriminator(1).to(device)\nrecon_layer = ReconciliationLayer(S_sku2cat_norm)\n\nopt_Gg = optim.Adam(Gg.parameters(),lr=LR)\nopt_Gl = optim.Adam(Gl.parameters(),lr=LR)\nopt_Dg = optim.Adam(Dg.parameters(),lr=LR)\nopt_Dl = optim.Adam(Dl.parameters(),lr=LR)\n\nx_seq_input = sales_cat.T.unsqueeze(0)\ny_real_global = sales_cat[:,-1].unsqueeze(0)\nprice_last = price_tensor_norm[:,-1].to(device)\npromo_last = promo_tensor[:,-1].to(device)\ndow_last = dow_tensor[:,-1,:].to(device)\ny_sku_true_norm = sales_tensor_norm[:,-1].unsqueeze(-1)\n\n# =======================\n# Training loop with causal loss\n# =======================\nprint(\"Starting training...\")\nfor epoch in range(EPOCHS):\n    # --- Global discriminator\n    for _ in range(N_CRITIC):\n        y_fake_global = Gg(x_seq_input)\n        D_real = Dg(y_real_global)\n        D_fake = Dg(y_fake_global.detach())\n        gp = gradient_penalty(Dg,y_real_global,y_fake_global.detach())\n        loss_Dg = -torch.mean(D_real)+torch.mean(D_fake)+10*gp\n        opt_Dg.zero_grad(); loss_Dg.backward(); opt_Dg.step()\n    y_fake_global = Gg(x_seq_input)\n    loss_Gg = -torch.mean(Dg(y_fake_global))\n    opt_Gg.zero_grad(); loss_Gg.backward(retain_graph=True); opt_Gg.step()\n\n    # --- Local generator & discriminator\n    y_fake_global_local = y_fake_global.detach()\n    y_fake_local_norm = Gl(y_fake_global_local, price_last, promo_last, dow_last, edge_index)\n    for _ in range(N_CRITIC):\n        D_real_local = Dl(y_sku_true_norm)\n        D_fake_local = Dl(y_fake_local_norm.detach())\n        gp_local = gradient_penalty(Dl,y_sku_true_norm,y_fake_local_norm.detach())\n        loss_Dl = -torch.mean(D_real_local)+torch.mean(D_fake_local)+10*gp_local\n        opt_Dl.zero_grad(); loss_Dl.backward(); opt_Dl.step()\n\n    adv_loss_local = -torch.mean(Dl(y_fake_local_norm))\n    mse_local = nn.MSELoss()(y_fake_local_norm.squeeze(-1), y_sku_true_norm.squeeze(-1))\n    y_recon_norm = recon_layer(y_fake_local_norm.squeeze(-1))\n    loss_coh = nn.MSELoss()(y_recon_norm, y_fake_global_local.squeeze(0))\n    loss_causal_val = causal_loss(y_fake_local_norm.squeeze(-1), price_last, promo_last, tau=CAUSAL_WEIGHT)\n    loss_Gl = ADV_WEIGHT_LOCAL*adv_loss_local + mse_local + COHERENCE_WEIGHT*loss_coh + loss_causal_val\n    opt_Gl.zero_grad(); loss_Gl.backward(); opt_Gl.step()\n\n    if (epoch+1)%10==0 or epoch==0:\n        with torch.no_grad():\n            rmse_sku = torch.sqrt(((y_sku_true_norm.squeeze(-1)-y_fake_local_norm.squeeze(-1))**2).mean()).item()\n            print(f\"Epoch {epoch+1}/{EPOCHS} | Dg {loss_Dg.item():.4f} | Dl {loss_Dl.item():.4f} | Gg {loss_Gg.item():.4f} | Gl {loss_Gl.item():.4f} | RMSE {rmse_sku:.4f} | Coh {loss_coh.item():.4f} | Causal {loss_causal_val.item():.4f}\")\n\nprint(\"Training finished.\")\n\n# =======================\n# Forecasting\n# =======================\nH = FORECAST_H\nx_sku = sales_tensor_norm.clone()\nx_cat = sales_cat.clone()\nprice_hist = price_tensor_norm.clone()\npromo_hist = promo_tensor.clone()\ndow_hist = dow_tensor.clone()\nfuture_local = []\nfuture_global = []\n\nwith torch.no_grad():\n    for step in range(H):\n        global_in = x_cat.T.unsqueeze(0)\n        g_pred = Gg(global_in).squeeze(0)\n        future_global.append(g_pred)\n        price_last_step = price_hist[:,-1]\n        promo_last_step = promo_hist[:,-1]\n        dow_last_step = dow_hist[:,-1,:]\n        local_pred = Gl(g_pred, price_last_step, promo_last_step, dow_last_step, edge_index).squeeze(-1)\n        future_local.append(local_pred)\n        x_sku = torch.cat([x_sku[:,1:], local_pred.unsqueeze(1)], dim=1)\n        next_cat = S_sku2cat_norm @ local_pred\n        x_cat = torch.cat([x_cat[:,1:], next_cat.unsqueeze(1)], dim=1)\n        price_hist = torch.cat([price_hist[:,1:], price_last_step.unsqueeze(1)], dim=1)\n        promo_hist = torch.cat([promo_hist[:,1:], promo_last_step.unsqueeze(1)], dim=1)\n        dow_hist = torch.cat([dow_hist[:,1:], dow_last_step.unsqueeze(1)], dim=1)\n\nfuture_local_norm = torch.stack(future_local,dim=1)\nfuture_local_raw = torch.expm1(future_local_norm*sales_std+sales_mean)\nprint('Forecast shape:', future_local_raw.shape)\n\n# =======================\n# Probabilistic evaluation (CRPS)\n# =======================\ndef crps(y_true, y_samples):\n    \"\"\"\n    y_true: (n_skus, H)\n    y_samples: (n_skus, H, n_samples)\n    \"\"\"\n    n_samples = y_samples.shape[2]\n    y_true_exp = y_true.unsqueeze(2).repeat(1,1,n_samples)\n    term1 = torch.mean(torch.abs(y_samples - y_true_exp), dim=2)\n    term2 = 0.5 * torch.mean(torch.abs(y_samples.unsqueeze(3) - y_samples.unsqueeze(2)), dim=(2,3))\n    return torch.mean(term1 - term2).item()\n\n# =======================\n# Evaluation functions\n# =======================\ndef compute_wrmsse(y_true, y_pred, S_matrix, scale_vec):\n    y_true_agg = S_matrix @ y_true\n    y_pred_agg = S_matrix @ y_pred\n    scale_agg = (S_matrix @ scale_vec[:,None])+1e-6\n    mse = ((y_true_agg - y_pred_agg)**2)/scale_agg\n    return torch.sqrt(mse.mean()).item()\n\ndef gan_quality_metrics(real, fake):\n    real_flat = real.detach().cpu().flatten()\n    fake_flat = fake.detach().cpu().flatten()\n    hist_real = np.histogram(real_flat, bins=50, density=True)[0]+1e-8\n    hist_fake = np.histogram(fake_flat, bins=50, density=True)[0]+1e-8\n    hist_real /= hist_real.sum(); hist_fake/=hist_fake.sum()\n    kl = np.sum(hist_real*np.log(hist_real/hist_fake))\n    wd = np.sum(np.abs(np.cumsum(hist_real)-np.cumsum(hist_fake)))\n    return kl, wd\n\ndef pinball_loss(y_true,y_pred,tau=0.5):\n    diff = y_true-y_pred\n    loss = torch.max(tau*diff,(tau-1)*diff)\n    return loss.mean().item()\n\n# =======================\n# Forecast evaluation\n# =======================\nwith torch.no_grad():\n    # WRMSSE / GAN metrics / Pinball\n    y_true_forecast = sales_tensor_raw[:,-FORECAST_H:]\n    y_pred_forecast = future_local_raw\n    scale_vec = sales_tensor_raw.var(dim=1)+1e-6\n\n    wrmsse_sku = compute_wrmsse(y_true_forecast, y_pred_forecast, torch.eye(n_skus).to(device), scale_vec)\n    wrmsse_cat = compute_wrmsse(y_true_forecast, y_pred_forecast, S_sku2cat_norm, scale_vec)\n    kl_div, wd = gan_quality_metrics(y_sku_true_norm, y_fake_local_norm)\n    pinball = pinball_loss(y_true_forecast, y_pred_forecast, tau=0.5)\n\n    # Probabilistic CRPS\n    n_samples = 10\n    future_samples = []\n    for s in range(n_samples):\n        x_sku_s = sales_tensor_norm.clone()\n        x_cat_s = sales_cat.clone()\n        price_hist_s = price_tensor_norm.clone()\n        promo_hist_s = promo_tensor.clone()\n        dow_hist_s = dow_tensor.clone()\n        step_forecast = []\n        for step in range(FORECAST_H):\n            g_pred_s = Gg(x_cat_s.T.unsqueeze(0)).squeeze(0)\n            local_pred_s = Gl(g_pred_s, price_hist_s[:,-1], promo_hist_s[:,-1], dow_hist_s[:,-1,:], edge_index).squeeze(-1)\n            step_forecast.append(local_pred_s)\n            x_sku_s = torch.cat([x_sku_s[:,1:], local_pred_s.unsqueeze(1)], dim=1)\n            x_cat_s = torch.cat([x_cat_s[:,1:], (S_sku2cat_norm @ local_pred_s).unsqueeze(1)], dim=1)\n            price_hist_s = torch.cat([price_hist_s[:,1:], price_hist_s[:,-1].unsqueeze(1)], dim=1)\n            promo_hist_s = torch.cat([promo_hist_s[:,1:], promo_hist_s[:,-1].unsqueeze(1)], dim=1)\n            dow_hist_s = torch.cat([dow_hist_s[:,1:], dow_hist_s[:,-1,:].unsqueeze(1)], dim=1)\n        future_samples.append(torch.stack(step_forecast, dim=1))\n    future_samples = torch.stack(future_samples, dim=2)\n    crps_score = crps(y_true_forecast, future_samples)\n\n    print(f\"WRMSSE SKU: {wrmsse_sku:.4f} | WRMSSE Cat: {wrmsse_cat:.4f}\")\n    print(f\"GAN KL: {kl_div:.4f} | Wasserstein: {wd:.4f}\")\n    print(f\"Pinball Loss (tau=0.5): {pinball:.4f}\")\n    print(f\"CRPS (multi-scenario): {crps_score:.4f}\")\n\n\n    # --- Add store-level MAE / MAPE ---\n    store_ids = sales_sample['store_id'].values\n    unique_stores = np.unique(store_ids)\n    \n    mae_per_sku = torch.mean(torch.abs(y_true_forecast - y_pred_forecast), dim=1)\n    mape_per_sku = torch.mean(torch.abs((y_true_forecast - y_pred_forecast) / (y_true_forecast + 1e-6)), dim=1) * 100\n    \n    mae_per_store = {}\n    mape_per_store = {}\n    for s in unique_stores:\n        idxs = np.where(store_ids == s)[0]\n        mae_per_store[s] = mae_per_sku[idxs].mean().item()\n        mape_per_store[s] = mape_per_sku[idxs].mean().item()\n    \n    print(\"MAE per store:\", mae_per_store)\n    print(\"MAPE per store:\", mape_per_store)\n\n\n# =======================\n# Plot 5 random SKUs\n# =======================\nsample_idx = np.random.choice(n_skus,5,replace=False)\nfig, axs = plt.subplots(5,1,figsize=(10,12))\nfor i, idx in enumerate(sample_idx):\n    axs[i].plot(np.arange(LAST_N_DAYS), sales_tensor_raw[idx].cpu().numpy(), label='History')\n    axs[i].plot(np.arange(LAST_N_DAYS,LAST_N_DAYS+FORECAST_H), future_local_raw[idx].cpu().numpy(), label='Forecast')\n    axs[i].legend(); axs[i].set_title(f'SKU {idx}')\nplt.tight_layout(); plt.show()\n\n\n# %% [code]\n","metadata":{"_uuid":"97edeb46-27e0-4b61-90ee-94de02b00899","_cell_guid":"1f0e3740-8b7a-4771-ad6d-e9f26b714c19","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null}]}